{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[chapter_mnist_basics]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood: Training a Digit Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having seen what it looks like to actually train a variety of models in Chapter 2, let’s now look under the hood and see exactly what is going on. We’ll start by using computer vision to introduce fundamental tools and concepts for deep learning.\n",
    "\n",
    "To be exact, we'll discuss the roles of arrays and tensors and of broadcasting, a powerful technique for using them expressively. We'll explain stochastic gradient descent (SGD), the mechanism for learning by updating weights automatically. We'll discuss the choice of a loss function for our basic classification task, and the role of mini-batches. We'll also describe the math that a basic neural network is actually doing. Finally, we'll put all these pieces together.\n",
    "\n",
    "In future chapters we’ll do deep dives into other applications as well, and see how these concepts and tools generalize. But this chapter is about laying foundation stones. To be frank, that also makes this one of the hardest chapters, because of how these concepts all depend on each other. Like an arch, all the stones need to be in place for the structure to stay up. Also like an arch, once that happens, it's a powerful structure that can support other things. But it requires some patience to assemble.\n",
    "\n",
    "Let's begin. The first step is to consider how images are represented in a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixels: The Foundations of Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what happens in a computer vision model, we first have to understand how computers handle images. We'll use one of the most famous datasets in computer vision, [MNIST](https://en.wikipedia.org/wiki/MNIST_database), for our experiments. MNIST contains images of handwritten digits, collected by the National Institute of Standards and Technology and collated into a machine learning dataset by Yann Lecun and his colleagues. Lecun used MNIST in 1998 in [Lenet-5](http://yann.lecun.com/exdb/lenet/), the first computer system to demonstrate practically useful recognition of handwritten digit sequences. This was one of the most important breakthroughs in the history of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidebar: Tenacity and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The story of deep learning is one of tenacity and grit by a handful of dedicated researchers. After early hopes (and hype!) neural networks went out of favor in the 1990's and 2000's, and just a handful of researchers kept trying to make them work well. Three of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded the highest honor in computer science, the Turing Award (generally considered the \"Nobel Prize of computer science\"), in 2018 after triumphing despite the deep skepticism and disinterest of the wider machine learning and statistics community.\n",
    "\n",
    "Geoff Hinton has told of how even academic papers showing dramatically better results than anything previously published would be rejected by top journals and conferences, just because they used a neural network. Yann Lecun's work on convolutional neural networks, which we will study in the next section, showed that these models could read handwritten text—something that had never been achieved before. However, his breakthrough was ignored by most researchers, even as it was used commercially to read 10% of the checks in the US!\n",
    "\n",
    "In addition to these three Turing Award winners, there are many other researchers who have battled to get us to where we are today. For instance, Jurgen Schmidhuber (who many believe should have shared in the Turing Award) pioneered many important ideas, including working with his student Sepp Hochreiter on the long short-term memory (LSTM) architecture (widely used for speech recognition and other text modeling tasks, and used in the IMDb example in <<chapter_intro>>). Perhaps most important of all, Paul Werbos in 1974 invented back-propagation for neural networks, the technique shown in this chapter and used universally for training neural networks ([Werbos 1994](https://books.google.com/books/about/The_Roots_of_Backpropagation.html?id=WdR3OOM2gBwC)). His development was almost entirely ignored for decades, but today it is considered the most important foundation of modern AI.\n",
    "\n",
    "There is a lesson here for all of us! On your deep learning journey you will face many obstacles, both technical, and (even more difficult) posed by people around you who don't believe you'll be successful. There's one *guaranteed* way to fail, and that's to stop trying. We've seen that the only consistent trait amongst every fast.ai student that's gone on to be a world-class practitioner is that they are all very tenacious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End sidebar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this initial tutorial we are just going to try to create a model that can classify any image as a 3 or a 7. So let's download a sample of MNIST that contains images of just these digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs.MNIST_SAMPLE??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "untar_data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what's in this directory by using `ls`, a method added by fastai. This method returns an object of a special fastai class called `L`, which has all the same functionality of Python's built-in `list`, plus a lot more. One of its handy features is that, when printed, it displays the count of items, before listing the items themselves (if there are more than 10 items, it just shows the first few):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(path.ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('train'),Path('valid'),Path('labels.csv')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset follows a common layout for machine learning datasets: separate folders for the training set and the validation set (and/or test set). Let's see what's inside the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('train/7'),Path('train/3')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a folder of 3s, and a folder of 7s. In machine learning parlance, we say that \"3\" and \"7\" are the *labels* (or targets) in this dataset. Let's take a look in one of these folders (using `sorted` to ensure we all get the same order of files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threes = (path/'train'/'3').ls().sorted()\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "threes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect, it's full of image files. Let’s take a look at one now. Here’s an image of a handwritten number 3, taken from the famous MNIST dataset of handwritten numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4nM3Or0sDcRjH8c/pgrfBVBjCgibThiKIyTWbWF1bORhGwxARxH/AbtW0JoIGwzXRYhJhtuFY2q1ocLgbe3sGReTuuWbwkx6+r+/zQ/pncX6q+YOldSe6nG3dn8U/rTQ70L8FCGJUewvxl7NTmezNb8xIkvKugr1HSeMP6SrWOVkoTEuSyh0Gm2n3hQyObMnXnxkempRrvgD+gokzwxFAr7U7YXHZ8x4A/Dl7rbu6D2yl3etcw/F3nZgfRVI7rXM7hMUUqzzBec427x26rkmlkzEEa4nnRqnSOH2F0UUx0ePzlbuqMXAHgN6GY9if5xP8dmtHFfwjuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F79A0FD9400>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3_path = threes[1]\n",
    "im3 = Image.open(im3_path)\n",
    "im3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the `Image` class from the *Python Imaging Library* (PIL), which is the most widely used Python package for opening, manipulating, and viewing images. Jupyter knows about PIL images, so it displays the image for us automatically.\n",
    "\n",
    "In a computer, everything is represented as a number. To view the numbers that make up this image, we have to convert it to a *NumPy array* or a *PyTorch tensor*. For instance, here's what a section of the image looks like, converted to a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  29],\n",
       "       [  0,   0,   0,  48, 166, 224],\n",
       "       [  0,  93, 244, 249, 253, 187],\n",
       "       [  0, 107, 253, 253, 230,  48],\n",
       "       [  0,   3,  20,  20,  15,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(im3)[4:10,4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `4:10` indicates we requested the rows from index 4 (included) to 10 (not included) and the same for the columns. NumPy indexes from top to bottom and left to right, so this section is located in the top-left corner of the image. Here's the same thing as a PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  29],\n",
       "        [  0,   0,   0,  48, 166, 224],\n",
       "        [  0,  93, 244, 249, 253, 187],\n",
       "        [  0, 107, 253, 253, 230,  48],\n",
       "        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(im3)[4:10,4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can slice the array to pick just the part with the top of the digit in it, and then use a Pandas DataFrame to color-code the values using a gradient, which shows us clearly how the image is created from the pixel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6f747_row0_col0, #T_6f747_row0_col1, #T_6f747_row0_col2, #T_6f747_row0_col3, #T_6f747_row0_col4, #T_6f747_row0_col5, #T_6f747_row0_col6, #T_6f747_row0_col7, #T_6f747_row0_col8, #T_6f747_row0_col9, #T_6f747_row0_col10, #T_6f747_row0_col11, #T_6f747_row0_col12, #T_6f747_row0_col13, #T_6f747_row0_col14, #T_6f747_row0_col15, #T_6f747_row0_col16, #T_6f747_row0_col17, #T_6f747_row1_col0, #T_6f747_row1_col1, #T_6f747_row1_col2, #T_6f747_row1_col3, #T_6f747_row1_col4, #T_6f747_row1_col15, #T_6f747_row1_col16, #T_6f747_row1_col17, #T_6f747_row2_col0, #T_6f747_row2_col1, #T_6f747_row2_col2, #T_6f747_row2_col15, #T_6f747_row2_col16, #T_6f747_row2_col17, #T_6f747_row3_col0, #T_6f747_row3_col15, #T_6f747_row3_col16, #T_6f747_row3_col17, #T_6f747_row4_col0, #T_6f747_row4_col6, #T_6f747_row4_col7, #T_6f747_row4_col8, #T_6f747_row4_col9, #T_6f747_row4_col10, #T_6f747_row4_col15, #T_6f747_row4_col16, #T_6f747_row4_col17, #T_6f747_row5_col0, #T_6f747_row5_col5, #T_6f747_row5_col6, #T_6f747_row5_col7, #T_6f747_row5_col8, #T_6f747_row5_col9, #T_6f747_row5_col15, #T_6f747_row5_col16, #T_6f747_row5_col17, #T_6f747_row6_col0, #T_6f747_row6_col1, #T_6f747_row6_col2, #T_6f747_row6_col3, #T_6f747_row6_col4, #T_6f747_row6_col5, #T_6f747_row6_col6, #T_6f747_row6_col7, #T_6f747_row6_col8, #T_6f747_row6_col9, #T_6f747_row6_col14, #T_6f747_row6_col15, #T_6f747_row6_col16, #T_6f747_row6_col17, #T_6f747_row7_col0, #T_6f747_row7_col1, #T_6f747_row7_col2, #T_6f747_row7_col3, #T_6f747_row7_col4, #T_6f747_row7_col5, #T_6f747_row7_col6, #T_6f747_row7_col13, #T_6f747_row7_col14, #T_6f747_row7_col15, #T_6f747_row7_col16, #T_6f747_row7_col17, #T_6f747_row8_col0, #T_6f747_row8_col1, #T_6f747_row8_col2, #T_6f747_row8_col3, #T_6f747_row8_col4, #T_6f747_row8_col13, #T_6f747_row8_col14, #T_6f747_row8_col15, #T_6f747_row8_col16, #T_6f747_row8_col17, #T_6f747_row9_col0, #T_6f747_row9_col1, #T_6f747_row9_col2, #T_6f747_row9_col3, #T_6f747_row9_col4, #T_6f747_row9_col16, #T_6f747_row9_col17, #T_6f747_row10_col0, #T_6f747_row10_col1, #T_6f747_row10_col2, #T_6f747_row10_col3, #T_6f747_row10_col4, #T_6f747_row10_col5, #T_6f747_row10_col6, #T_6f747_row10_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #ffffff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row1_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #efefef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row1_col6, #T_6f747_row1_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #7c7c7c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row1_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4a4a4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row1_col8, #T_6f747_row1_col9, #T_6f747_row1_col10, #T_6f747_row2_col5, #T_6f747_row2_col6, #T_6f747_row2_col7, #T_6f747_row2_col11, #T_6f747_row2_col12, #T_6f747_row2_col13, #T_6f747_row3_col4, #T_6f747_row3_col12, #T_6f747_row3_col13, #T_6f747_row4_col1, #T_6f747_row4_col2, #T_6f747_row4_col3, #T_6f747_row4_col12, #T_6f747_row4_col13, #T_6f747_row5_col12, #T_6f747_row6_col11, #T_6f747_row9_col11, #T_6f747_row10_col11, #T_6f747_row10_col12, #T_6f747_row10_col13, #T_6f747_row10_col14, #T_6f747_row10_col15, #T_6f747_row10_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row1_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #606060;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row1_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4d4d4d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row1_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bbbbbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row2_col3 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e4e4e4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row2_col4, #T_6f747_row8_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #6b6b6b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row2_col8, #T_6f747_row2_col14, #T_6f747_row3_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #171717;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row2_col9, #T_6f747_row3_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4b4b4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row2_col10, #T_6f747_row7_col10, #T_6f747_row8_col8, #T_6f747_row8_col10, #T_6f747_row9_col8, #T_6f747_row9_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #010101;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row3_col1 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #272727;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row3_col2 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #0a0a0a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row3_col3 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #050505;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row3_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #333333;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row3_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e6e6e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row3_col7, #T_6f747_row3_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fafafa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row3_col8 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fbfbfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row3_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fdfdfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row4_col4 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #1b1b1b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row4_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e0e0e0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row4_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4e4e4e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row4_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #767676;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row5_col1 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fcfcfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row5_col2, #T_6f747_row5_col3 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f6f6f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row5_col4, #T_6f747_row7_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f8f8f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row5_col10, #T_6f747_row10_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e8e8e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row5_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #222222;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row5_col13, #T_6f747_row6_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #090909;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row5_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d0d0d0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row6_col10, #T_6f747_row7_col11, #T_6f747_row9_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #060606;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row6_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #979797;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row7_col8 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #b6b6b6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row7_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #252525;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row7_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #999999;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row8_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f9f9f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row8_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #101010;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row8_col9, #T_6f747_row9_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #020202;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row8_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #545454;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row8_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f1f1f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row9_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f7f7f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row9_col7 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #030303;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row9_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #181818;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row9_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #303030;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row9_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #a9a9a9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6f747_row9_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fefefe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row10_col8, #T_6f747_row10_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bababa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6f747_row10_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #393939;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6f747_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >0</th>\n",
       "      <th class=\"col_heading level0 col1\" >1</th>\n",
       "      <th class=\"col_heading level0 col2\" >2</th>\n",
       "      <th class=\"col_heading level0 col3\" >3</th>\n",
       "      <th class=\"col_heading level0 col4\" >4</th>\n",
       "      <th class=\"col_heading level0 col5\" >5</th>\n",
       "      <th class=\"col_heading level0 col6\" >6</th>\n",
       "      <th class=\"col_heading level0 col7\" >7</th>\n",
       "      <th class=\"col_heading level0 col8\" >8</th>\n",
       "      <th class=\"col_heading level0 col9\" >9</th>\n",
       "      <th class=\"col_heading level0 col10\" >10</th>\n",
       "      <th class=\"col_heading level0 col11\" >11</th>\n",
       "      <th class=\"col_heading level0 col12\" >12</th>\n",
       "      <th class=\"col_heading level0 col13\" >13</th>\n",
       "      <th class=\"col_heading level0 col14\" >14</th>\n",
       "      <th class=\"col_heading level0 col15\" >15</th>\n",
       "      <th class=\"col_heading level0 col16\" >16</th>\n",
       "      <th class=\"col_heading level0 col17\" >17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6f747_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col9\" class=\"data row0 col9\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col10\" class=\"data row0 col10\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col12\" class=\"data row0 col12\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col13\" class=\"data row0 col13\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col14\" class=\"data row0 col14\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col15\" class=\"data row0 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col16\" class=\"data row0 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6f747_row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_6f747_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_6f747_row1_col5\" class=\"data row1 col5\" >29</td>\n",
       "      <td id=\"T_6f747_row1_col6\" class=\"data row1 col6\" >150</td>\n",
       "      <td id=\"T_6f747_row1_col7\" class=\"data row1 col7\" >195</td>\n",
       "      <td id=\"T_6f747_row1_col8\" class=\"data row1 col8\" >254</td>\n",
       "      <td id=\"T_6f747_row1_col9\" class=\"data row1 col9\" >255</td>\n",
       "      <td id=\"T_6f747_row1_col10\" class=\"data row1 col10\" >254</td>\n",
       "      <td id=\"T_6f747_row1_col11\" class=\"data row1 col11\" >176</td>\n",
       "      <td id=\"T_6f747_row1_col12\" class=\"data row1 col12\" >193</td>\n",
       "      <td id=\"T_6f747_row1_col13\" class=\"data row1 col13\" >150</td>\n",
       "      <td id=\"T_6f747_row1_col14\" class=\"data row1 col14\" >96</td>\n",
       "      <td id=\"T_6f747_row1_col15\" class=\"data row1 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row1_col16\" class=\"data row1 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row1_col17\" class=\"data row1 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6f747_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row2_col3\" class=\"data row2 col3\" >48</td>\n",
       "      <td id=\"T_6f747_row2_col4\" class=\"data row2 col4\" >166</td>\n",
       "      <td id=\"T_6f747_row2_col5\" class=\"data row2 col5\" >224</td>\n",
       "      <td id=\"T_6f747_row2_col6\" class=\"data row2 col6\" >253</td>\n",
       "      <td id=\"T_6f747_row2_col7\" class=\"data row2 col7\" >253</td>\n",
       "      <td id=\"T_6f747_row2_col8\" class=\"data row2 col8\" >234</td>\n",
       "      <td id=\"T_6f747_row2_col9\" class=\"data row2 col9\" >196</td>\n",
       "      <td id=\"T_6f747_row2_col10\" class=\"data row2 col10\" >253</td>\n",
       "      <td id=\"T_6f747_row2_col11\" class=\"data row2 col11\" >253</td>\n",
       "      <td id=\"T_6f747_row2_col12\" class=\"data row2 col12\" >253</td>\n",
       "      <td id=\"T_6f747_row2_col13\" class=\"data row2 col13\" >253</td>\n",
       "      <td id=\"T_6f747_row2_col14\" class=\"data row2 col14\" >233</td>\n",
       "      <td id=\"T_6f747_row2_col15\" class=\"data row2 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row2_col16\" class=\"data row2 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row2_col17\" class=\"data row2 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6f747_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row3_col1\" class=\"data row3 col1\" >93</td>\n",
       "      <td id=\"T_6f747_row3_col2\" class=\"data row3 col2\" >244</td>\n",
       "      <td id=\"T_6f747_row3_col3\" class=\"data row3 col3\" >249</td>\n",
       "      <td id=\"T_6f747_row3_col4\" class=\"data row3 col4\" >253</td>\n",
       "      <td id=\"T_6f747_row3_col5\" class=\"data row3 col5\" >187</td>\n",
       "      <td id=\"T_6f747_row3_col6\" class=\"data row3 col6\" >46</td>\n",
       "      <td id=\"T_6f747_row3_col7\" class=\"data row3 col7\" >10</td>\n",
       "      <td id=\"T_6f747_row3_col8\" class=\"data row3 col8\" >8</td>\n",
       "      <td id=\"T_6f747_row3_col9\" class=\"data row3 col9\" >4</td>\n",
       "      <td id=\"T_6f747_row3_col10\" class=\"data row3 col10\" >10</td>\n",
       "      <td id=\"T_6f747_row3_col11\" class=\"data row3 col11\" >194</td>\n",
       "      <td id=\"T_6f747_row3_col12\" class=\"data row3 col12\" >253</td>\n",
       "      <td id=\"T_6f747_row3_col13\" class=\"data row3 col13\" >253</td>\n",
       "      <td id=\"T_6f747_row3_col14\" class=\"data row3 col14\" >233</td>\n",
       "      <td id=\"T_6f747_row3_col15\" class=\"data row3 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row3_col16\" class=\"data row3 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row3_col17\" class=\"data row3 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6f747_row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col1\" class=\"data row4 col1\" >107</td>\n",
       "      <td id=\"T_6f747_row4_col2\" class=\"data row4 col2\" >253</td>\n",
       "      <td id=\"T_6f747_row4_col3\" class=\"data row4 col3\" >253</td>\n",
       "      <td id=\"T_6f747_row4_col4\" class=\"data row4 col4\" >230</td>\n",
       "      <td id=\"T_6f747_row4_col5\" class=\"data row4 col5\" >48</td>\n",
       "      <td id=\"T_6f747_row4_col6\" class=\"data row4 col6\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col7\" class=\"data row4 col7\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col8\" class=\"data row4 col8\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col9\" class=\"data row4 col9\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col10\" class=\"data row4 col10\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col11\" class=\"data row4 col11\" >192</td>\n",
       "      <td id=\"T_6f747_row4_col12\" class=\"data row4 col12\" >253</td>\n",
       "      <td id=\"T_6f747_row4_col13\" class=\"data row4 col13\" >253</td>\n",
       "      <td id=\"T_6f747_row4_col14\" class=\"data row4 col14\" >156</td>\n",
       "      <td id=\"T_6f747_row4_col15\" class=\"data row4 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col16\" class=\"data row4 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row4_col17\" class=\"data row4 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_6f747_row5_col0\" class=\"data row5 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col1\" class=\"data row5 col1\" >3</td>\n",
       "      <td id=\"T_6f747_row5_col2\" class=\"data row5 col2\" >20</td>\n",
       "      <td id=\"T_6f747_row5_col3\" class=\"data row5 col3\" >20</td>\n",
       "      <td id=\"T_6f747_row5_col4\" class=\"data row5 col4\" >15</td>\n",
       "      <td id=\"T_6f747_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col6\" class=\"data row5 col6\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col7\" class=\"data row5 col7\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col8\" class=\"data row5 col8\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col9\" class=\"data row5 col9\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col10\" class=\"data row5 col10\" >43</td>\n",
       "      <td id=\"T_6f747_row5_col11\" class=\"data row5 col11\" >224</td>\n",
       "      <td id=\"T_6f747_row5_col12\" class=\"data row5 col12\" >253</td>\n",
       "      <td id=\"T_6f747_row5_col13\" class=\"data row5 col13\" >245</td>\n",
       "      <td id=\"T_6f747_row5_col14\" class=\"data row5 col14\" >74</td>\n",
       "      <td id=\"T_6f747_row5_col15\" class=\"data row5 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col16\" class=\"data row5 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row5_col17\" class=\"data row5 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_6f747_row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col4\" class=\"data row6 col4\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col6\" class=\"data row6 col6\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col7\" class=\"data row6 col7\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col8\" class=\"data row6 col8\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col9\" class=\"data row6 col9\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col10\" class=\"data row6 col10\" >249</td>\n",
       "      <td id=\"T_6f747_row6_col11\" class=\"data row6 col11\" >253</td>\n",
       "      <td id=\"T_6f747_row6_col12\" class=\"data row6 col12\" >245</td>\n",
       "      <td id=\"T_6f747_row6_col13\" class=\"data row6 col13\" >126</td>\n",
       "      <td id=\"T_6f747_row6_col14\" class=\"data row6 col14\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col15\" class=\"data row6 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col16\" class=\"data row6 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row6_col17\" class=\"data row6 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_6f747_row7_col0\" class=\"data row7 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col4\" class=\"data row7 col4\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col5\" class=\"data row7 col5\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col6\" class=\"data row7 col6\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col7\" class=\"data row7 col7\" >14</td>\n",
       "      <td id=\"T_6f747_row7_col8\" class=\"data row7 col8\" >101</td>\n",
       "      <td id=\"T_6f747_row7_col9\" class=\"data row7 col9\" >223</td>\n",
       "      <td id=\"T_6f747_row7_col10\" class=\"data row7 col10\" >253</td>\n",
       "      <td id=\"T_6f747_row7_col11\" class=\"data row7 col11\" >248</td>\n",
       "      <td id=\"T_6f747_row7_col12\" class=\"data row7 col12\" >124</td>\n",
       "      <td id=\"T_6f747_row7_col13\" class=\"data row7 col13\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col14\" class=\"data row7 col14\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col15\" class=\"data row7 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col16\" class=\"data row7 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row7_col17\" class=\"data row7 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_6f747_row8_col0\" class=\"data row8 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col3\" class=\"data row8 col3\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col4\" class=\"data row8 col4\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col5\" class=\"data row8 col5\" >11</td>\n",
       "      <td id=\"T_6f747_row8_col6\" class=\"data row8 col6\" >166</td>\n",
       "      <td id=\"T_6f747_row8_col7\" class=\"data row8 col7\" >239</td>\n",
       "      <td id=\"T_6f747_row8_col8\" class=\"data row8 col8\" >253</td>\n",
       "      <td id=\"T_6f747_row8_col9\" class=\"data row8 col9\" >253</td>\n",
       "      <td id=\"T_6f747_row8_col10\" class=\"data row8 col10\" >253</td>\n",
       "      <td id=\"T_6f747_row8_col11\" class=\"data row8 col11\" >187</td>\n",
       "      <td id=\"T_6f747_row8_col12\" class=\"data row8 col12\" >30</td>\n",
       "      <td id=\"T_6f747_row8_col13\" class=\"data row8 col13\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col14\" class=\"data row8 col14\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col15\" class=\"data row8 col15\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col16\" class=\"data row8 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row8_col17\" class=\"data row8 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_6f747_row9_col0\" class=\"data row9 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "      <td id=\"T_6f747_row9_col4\" class=\"data row9 col4\" >0</td>\n",
       "      <td id=\"T_6f747_row9_col5\" class=\"data row9 col5\" >16</td>\n",
       "      <td id=\"T_6f747_row9_col6\" class=\"data row9 col6\" >248</td>\n",
       "      <td id=\"T_6f747_row9_col7\" class=\"data row9 col7\" >250</td>\n",
       "      <td id=\"T_6f747_row9_col8\" class=\"data row9 col8\" >253</td>\n",
       "      <td id=\"T_6f747_row9_col9\" class=\"data row9 col9\" >253</td>\n",
       "      <td id=\"T_6f747_row9_col10\" class=\"data row9 col10\" >253</td>\n",
       "      <td id=\"T_6f747_row9_col11\" class=\"data row9 col11\" >253</td>\n",
       "      <td id=\"T_6f747_row9_col12\" class=\"data row9 col12\" >232</td>\n",
       "      <td id=\"T_6f747_row9_col13\" class=\"data row9 col13\" >213</td>\n",
       "      <td id=\"T_6f747_row9_col14\" class=\"data row9 col14\" >111</td>\n",
       "      <td id=\"T_6f747_row9_col15\" class=\"data row9 col15\" >2</td>\n",
       "      <td id=\"T_6f747_row9_col16\" class=\"data row9 col16\" >0</td>\n",
       "      <td id=\"T_6f747_row9_col17\" class=\"data row9 col17\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6f747_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_6f747_row10_col0\" class=\"data row10 col0\" >0</td>\n",
       "      <td id=\"T_6f747_row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "      <td id=\"T_6f747_row10_col2\" class=\"data row10 col2\" >0</td>\n",
       "      <td id=\"T_6f747_row10_col3\" class=\"data row10 col3\" >0</td>\n",
       "      <td id=\"T_6f747_row10_col4\" class=\"data row10 col4\" >0</td>\n",
       "      <td id=\"T_6f747_row10_col5\" class=\"data row10 col5\" >0</td>\n",
       "      <td id=\"T_6f747_row10_col6\" class=\"data row10 col6\" >0</td>\n",
       "      <td id=\"T_6f747_row10_col7\" class=\"data row10 col7\" >43</td>\n",
       "      <td id=\"T_6f747_row10_col8\" class=\"data row10 col8\" >98</td>\n",
       "      <td id=\"T_6f747_row10_col9\" class=\"data row10 col9\" >98</td>\n",
       "      <td id=\"T_6f747_row10_col10\" class=\"data row10 col10\" >208</td>\n",
       "      <td id=\"T_6f747_row10_col11\" class=\"data row10 col11\" >253</td>\n",
       "      <td id=\"T_6f747_row10_col12\" class=\"data row10 col12\" >253</td>\n",
       "      <td id=\"T_6f747_row10_col13\" class=\"data row10 col13\" >253</td>\n",
       "      <td id=\"T_6f747_row10_col14\" class=\"data row10 col14\" >253</td>\n",
       "      <td id=\"T_6f747_row10_col15\" class=\"data row10 col15\" >187</td>\n",
       "      <td id=\"T_6f747_row10_col16\" class=\"data row10 col16\" >22</td>\n",
       "      <td id=\"T_6f747_row10_col17\" class=\"data row10 col17\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f79a0640a60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "im3_t = tensor(im3)\n",
    "df = pd.DataFrame(im3_t[4:15,4:22])\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the background white pixels are stored as the number 0, black is the number 255, and shades of gray are between the two. The entire image contains 28 pixels across and 28 pixels down, for a total of 784 pixels. (This is much smaller than an image that you would get from a phone camera, which has millions of pixels, but is a convenient size for our initial learning and experiments. We will build up to bigger, full-color images soon.)\n",
    "\n",
    "So, now you've seen what an image looks like to a computer, let's recall our goal: create a model that can recognize 3s and 7s. How might you go about getting a computer to do that?\n",
    "\n",
    "> Warning: Stop and Think!: Before you read on, take a moment to think about how a computer might be able to recognize these two different digits. What kinds of features might it be able to look at? How might it be able to identify these features? How could it combine them together? Learning works best when you try to solve problems yourself, rather than just reading somebody else's answers; so step away from this book for a few minutes, grab a piece of paper and pen, and jot some ideas down…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Try: Pixel Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here is a first idea: how about we find the average pixel value for every pixel of the 3s, then do the same for the 7s. This will give us two group averages, defining what we might call the \"ideal\" 3 and 7. Then, to classify an image as one digit or the other, we see which of these two ideal digits the image is most similar to. This certainly seems like it should be better than nothing, so it will make a good baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Baseline: A simple model which you are confident should perform reasonably well. It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline. Without starting with a sensible baseline, it is very difficult to know whether your super-fancy models are actually any good. One good approach to creating a baseline is doing what we have done here: think of a simple, easy-to-implement model. Another good approach is to search around to find other people that have solved similar problems to yours, and download and run their code on your dataset. Ideally, try both of these!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step one for our simple model is to get the average of pixel values for each of our two groups. In the process of doing this, we will learn a lot of neat Python numeric programming tricks!\n",
    "\n",
    "Let's create a tensor containing all of our 3s stacked together. We already know how to create a tensor containing a single image. To create a tensor containing all the images in a directory, we will first use a Python list comprehension to create a plain list of the single image tensors.\n",
    "\n",
    "We will use Jupyter to do some little checks of our work along the way—in this case, making sure that the number of returned items seems reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "len(three_tensors),len(seven_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note: List Comprehensions: List and dictionary comprehensions are a wonderful feature of Python. Many Python programmers use them every day, including the authors of this book—they are part of \"idiomatic Python.\" But programmers coming from other languages may have never seen them before. There are a lot of great tutorials just a web search away, so we won't spend a long time discussing them now. Here is a quick explanation and example to get you started. A list comprehension looks like this: `new_list = [f(o) for o in a_list if o>0]`. This will return every element of `a_list` that is greater than 0, after passing it to the function `f`. There are three parts here: the collection you are iterating over (`a_list`), an optional filter (`if o>0`), and something to do to each element (`f(o)`). It's not only shorter to write but way faster than the alternative ways of creating the same list with a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also check that one of the images looks okay. Since we now have tensors (which Jupyter by default will print as values), rather than PIL images (which Jupyter by default will display as images), we need to use fastai's `show_image` function to display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(three_tensors[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this we first combine all the images in this list into a single three-dimensional tensor. The most common way to describe such a tensor is to call it a *rank-3 tensor*. We often need to stack up individual tensors in a collection into a single tensor. Unsurprisingly, PyTorch comes with a function called `stack` that we can use for this purpose.\n",
    "\n",
    "Some operations in PyTorch, such as taking a mean, require us to *cast* our integer types to float types. Since we'll be needing this later, we'll also cast our stacked tensor to `float` now. Casting in PyTorch is as simple as typing the name of the type you wish to cast to, and treating it as a method.\n",
    "\n",
    "Generally when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most important attribute of a tensor is its *shape*. This tells you the length of each axis. In this case, we can see that we have 6,131 images, each of size 28×28 pixels. There is nothing specifically about this tensor that says that the first axis is the number of images, the second is the height, and the third is the width—the semantics of a tensor are entirely up to us, and how we construct it. As far as PyTorch is concerned, it is just a bunch of numbers in memory.\n",
    "\n",
    "The *length* of a tensor's shape is its rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stacked_threes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really important for you to commit to memory and practice these bits of tensor jargon: _rank_ is the number of axes or dimensions in a tensor; _shape_ is the size of each axis of a tensor.\n",
    "\n",
    "> A: Watch out because the term \"dimension\" is sometimes used in two ways. Consider that we live in \"three-dimensonal space\" where a physical position can be described by a 3-vector `v`. But according to PyTorch, the attribute `v.ndim` (which sure looks like the \"number of dimensions\" of `v`) equals one, not three! Why? Because `v` is a vector, which is a tensor of rank one, meaning that it has only one _axis_ (even if that axis has a length of three). In other words, sometimes dimension is used for the size of an axis (\"space is three-dimensional\"); other times, it is used for the rank, or the number of axes (\"a matrix has two dimensions\"). When confused, I find it helpful to translate all statements into terms of rank, axis, and length, which are unambiguous terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a tensor's rank directly with `ndim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute what the ideal 3 looks like. We calculate the mean of all the image tensors by taking the mean along dimension 0 of our stacked, rank-3 tensor. This is the dimension that indexes over all the images.\n",
    "\n",
    "In other words, for every pixel position, this will compute the average of that pixel over all images. The result will be one value for every pixel position, or a single image. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this dataset, this is the ideal number 3! (You may not like it, but this is what peak number 3 performance looks like.) You can see how it's very dark where all the images agree it should be dark, but it becomes wispy and blurry where the images disagree. \n",
    "\n",
    "Let's do the same thing for the 7s, but put all the steps together at once to save some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1145)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_sevens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean7 = stacked_sevens.mean(0)\n",
    "show_image(mean7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pick an arbitrary 3 and measure its *distance* from our \"ideal digits.\"\n",
    "\n",
    "> stop: Stop and Think!: How would you calculate how similar a particular image is to each of our ideal digits? Remember to step away from this book and jot down some ideas before you move on! Research shows that recall and understanding improves dramatically when you are engaged with the learning process by solving problems, experimenting, and trying new ideas yourself\n",
    "\n",
    "Here's a sample 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_3 = stacked_threes[1]\n",
    "show_image(a_3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we determine its distance from our ideal 3? We can't just add up the differences between the pixels of this image and the ideal digit. Some differences will be positive while others will be negative, and these differences will cancel out, resulting in a situation where an image that is too dark in some places and too light in others might be shown as having zero total differences from the ideal. That would be misleading!\n",
    "\n",
    "To avoid this, there are two main ways data scientists measure distance in this context:\n",
    "\n",
    "- Take the mean of the *absolute value* of differences (absolute value is the function that replaces negative values with positive values). This is called the *mean absolute difference* or *L1 norm*\n",
    "- Take the mean of the *square* of differences (which makes everything positive) and then take the *square root* (which undoes the squaring). This is called the *root mean squared error* (RMSE) or *L2 norm*.\n",
    "\n",
    "> important: It's Okay to Have Forgotten Your Math: In this book we generally assume that you have completed high school math, and remember at least some of it... But everybody forgets some things! It all depends on what you happen to have had reason to practice in the meantime. Perhaps you have forgotten what a _square root_ is, or exactly how they work. No problem! Any time you come across a maths concept that is not explained fully in this book, don't just keep moving on; instead, stop and look it up. Make sure you understand the basic idea, how it works, and why we might be using it. One of the best places to refresh your understanding is Khan Academy. For instance, Khan Academy has a great [introduction to square roots](https://www.khanacademy.org/math/algebra/x2f8bb11595b61c86:rational-exponents-radicals/x2f8bb11595b61c86:radicals/v/understanding-square-roots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try both of these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1114), tensor(0.2021))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_3_abs = (a_3 - mean3).abs().mean()\n",
    "dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n",
    "dist_3_abs,dist_3_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_7_abs = (a_3 - mean7).abs().mean()\n",
    "dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n",
    "dist_7_abs,dist_7_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the distance between our 3 and the \"ideal\" 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides both of these as *loss functions*. You'll find these inside `torch.nn.functional`, which the PyTorch team recommends importing as `F` (and is available by default under that name in fastai):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `mse` stands for *mean squared error*, and `l1` refers to the standard mathematical jargon for *mean absolute value* (in math it's called the *L1 norm*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S: Intuitively, the difference between L1 norm and mean squared error (MSE) is that the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: When I first came across this \"L1\" thingie, I looked it up to see what on earth it meant. I found on Google that it is a _vector norm_ using _absolute value_, so looked up _vector norm_ and started reading: _Given a vector space V over a field F of the real or complex numbers, a norm on V is a nonnegative-valued any function p: V → \\[0,+∞) with the following properties: For all a ∈ F and all u, v ∈ V, p(u + v) ≤ p(u) + p(v)..._ Then I stopped reading. \"Ugh, I'll never understand math!\" I thought, for the thousandth time. Since then I've learned that every time these complex mathy bits of jargon come up in practice, it turns out I can replace them with a tiny bit of code! Like, the _L1 loss_ is just equal to `(a-b).abs().mean()`, where `a` and `b` are tensors. I guess mathy folks just think differently than me... I'll make sure in this book that every time some mathy jargon comes up, I'll give you the little bit of code it's equal to as well, and explain in common-sense terms what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just completed various mathematical operations on PyTorch tensors. If you've done some numeric programming in NumPy before, you may recognize these as being similar to NumPy arrays. Let's have a look at those two very important data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Arrays and PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NumPy](https://numpy.org/) is the most widely used library for scientific and numeric programming in Python. It provides very similar functionality and a very similar API to that provided by PyTorch; however, it does not support using the GPU or calculating gradients, which are both critical for deep learning. Therefore, in this book we will generally use PyTorch tensors instead of NumPy arrays, where possible.\n",
    "\n",
    "(Note that fastai adds some features to NumPy and PyTorch to make them a bit more similar to each other. If any code in this book doesn't work on your computer, it's possible that you forgot to include a line like this at the start of your notebook: `from fastai.vision.all import *`.)\n",
    "\n",
    "But what are arrays and tensors, and why should you care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is slow compared to many languages. Anything fast in Python, NumPy, or PyTorch is likely to be a wrapper for a compiled object written (and optimized) in another language—specifically C. In fact, **NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.**\n",
    "\n",
    "A NumPy array is a multidimensional table of data, with all items of the same type. Since that can be any type at all, they can even be arrays of arrays, with the innermost arrays potentially being different sizes—this is called a \"jagged array.\" By \"multidimensional table\" we mean, for instance, a list (dimension of one), a table or matrix (dimension of two), a \"table of tables\" or \"cube\" (dimension of three), and so forth. If the items are all of some simple type such as integer or float, then NumPy will store them as a compact C data structure in memory. This is where NumPy shines. NumPy has a wide variety of operators and methods that can run computations on these compact structures at the same speed as optimized C, because they are written in optimized C.\n",
    "\n",
    "A PyTorch tensor is nearly the same thing as a NumPy array, but with an additional restriction that unlocks some additional capabilities. It's the same in that it, too, is a multidimensional table of data, with all items of the same type. However, the restriction is that a tensor cannot use just any old type—it has to use a single basic numeric type for all components. For example, a PyTorch tensor cannot be jagged. It is always a regularly shaped multidimensional rectangular structure.\n",
    "\n",
    "The vast majority of methods and operators supported by NumPy on these structures are also supported by PyTorch, but PyTorch tensors have additional capabilities. One major capability is that these structures can live on the GPU, in which case their computation will be optimized for the GPU and can run much faster (given lots of values to work on). In addition, PyTorch can automatically calculate derivatives of these operations, including combinations of operations. As you'll see, it would be impossible to do deep learning in practice without this capability.\n",
    "\n",
    "> S: If you don't know what C is, don't worry as you won't need it at all. In a nutshell, it's a low-level  (low-level means more similar to the language that computers use internally) language that is very fast compared to Python. To take advantage of its speed while programming in Python, try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors.\n",
    "\n",
    "Perhaps the most important new coding skill for a Python programmer to learn is how to effectively use the array/tensor APIs. We will be showing lots more tricks later in this book, but here's a summary of the key things you need to know for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an array or tensor, pass a list (or list of lists, or list of lists of lists, etc.) to `array()` or `tensor()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2,3],[4,5,6]]\n",
    "arr = array (data)\n",
    "tns = tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr  # numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns  # pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the operations that follow are shown on tensors, but the syntax and results for NumPy arrays is identical.\n",
    "\n",
    "You can select a row (note that, like lists in Python, tensors are 0-indexed so 1 refers to the second row/column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or a column, by using `:` to indicate *all of the first axis* (we sometimes refer to the dimensions of tensors/arrays as *axes*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine these with Python slice syntax (`[start:end]` with `end` being excluded) to select part of a row or column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[1,1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can use the standard operators such as `+`, `-`, `*`, `/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have a type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And will automatically change type as needed, for example from `int` to `float`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5000, 3.0000, 4.5000],\n",
       "        [6.0000, 7.5000, 9.0000]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns*1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, is our baseline model any good? To quantify this, we must define a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Metrics Using Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is. For instance, we could use either of the functions we saw in the previous section, mean squared error, or mean absolute error, and take the average of them over the whole dataset. However, neither of these are numbers that are very understandable to most people; in practice, we normally use *accuracy* as the metric for classification models.\n",
    "\n",
    "As we've discussed, we want to calculate our metric over a *validation set*. This is so that we don't inadvertently overfit—that is, train a model to work well only on our training data. This is not really a risk with the pixel similarity model we're using here as a first try, since it has no trained components, but we'll use a validation set anyway to follow normal practices and to be ready for our second try later.\n",
    "\n",
    "To get a validation set we need to remove some of the data from training entirely, so it is not seen by the model at all. As it turns out, the creators of the MNIST dataset have already done this for us. Do you remember how there was a whole separate directory called *valid*? That's what this directory is for!\n",
    "\n",
    "So to start with, let's create tensors for our 3s and 7s from that directory. These are the tensors we will use to calculate a metric measuring the quality of our first-try model, which measures distance from an ideal image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
    "                            for o in (path/'valid'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255\n",
    "valid_3_tens.shape,valid_7_tens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to get in the habit of checking shapes as you go. Here we see two tensors, one representing the 3s validation set of 1,010 images of size 28×28, and one representing the 7s validation set of 1,028 images of size 28×28.\n",
    "\n",
    "We ultimately want to write a function, `is_3`, that will decide if an arbitrary image is a 3 or a 7. It will do this by deciding which of our two \"ideal digits\" this arbitrary image is closer to. For that we need to define a notion of distance—that is, a function that calculates the distance between two images.\n",
    "\n",
    "We can write a simple function that calculates the mean absolute error using an expression very similar to the one we wrote in the last section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1114)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mnist_distance(a,b): return (a-b).abs().mean((-1,-2))\n",
    "mnist_distance(a_3, mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1010, 28, 28])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same value we previously calculated for the distance between these two images, the ideal 3 `mean3` and the arbitrary sample 3 `a_3`, which are both single-image tensors with a shape of `[28,28]`.\n",
    "\n",
    "But in order to calculate a metric for overall accuracy, we will need to calculate the distance to the ideal 3 for _every_ image in the validation set. How do we do that calculation? We could write a loop over all of the single-image tensors that are stacked within our validation set tensor, `valid_3_tens`, which has a shape of `[1010,28,28]` representing 1,010 images. But there is a better way.\n",
    "\n",
    "Something very interesting happens when we take this exact same distance function, designed for comparing two single images, but pass in as an argument `valid_3_tens`, the tensor that represents the 3s validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1305, 0.1161, 0.1101,  ..., 0.1460, 0.1307, 0.1654]),\n",
       " torch.Size([1010]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_dist = mnist_distance(valid_3_tens, mean3)\n",
    "valid_3_dist, valid_3_dist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of complaining about shapes not matching, it returned the distance for every single image as a vector (i.e., a rank-1 tensor) of length 1,010 (the number of 3s in our validation set). How did that happen?\n",
    "\n",
    "Take another look at our function `mnist_distance`, and you'll see we have there the subtraction `(a-b)`. The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use *broadcasting*. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.\n",
    "\n",
    "After broadcasting so the two argument tensors have the same rank, PyTorch applies its usual logic for two tensors of the same rank: it performs the operation on each corresponding element of the two tensors, and returns the tensor result. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor([1,2,3]) + tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this case, PyTorch treats `mean3`, a rank-2 tensor representing a single image, as if it were 1,010 copies of the same image, and then subtracts each of those copies from each 3 in our validation set. What shape would you expect this tensor to have? Try to figure it out yourself before you look at the answer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1010, 28, 28])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(valid_3_tens-mean3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are calculating the difference between our \"ideal 3\" and each of the 1,010 3s in the validation set, for each of 28×28 images, resulting in the shape `[1010,28,28]`.\n",
    "\n",
    "There are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance:\n",
    "\n",
    "- PyTorch doesn't *actually* copy `mean3` 1,010 times. It *pretends* it were a tensor of that shape, but doesn't actually allocate any additional memory\n",
    "- It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!).\n",
    "\n",
    "This is true of all broadcasting and elementwise operations and functions done in PyTorch. *It's the most important technique for you to know to create efficient PyTorch code.*\n",
    "\n",
    "Next in `mnist_distance` we see `abs`. You might be able to guess now what this does when applied to a tensor. It applies the method to each individual element in the tensor, and returns a tensor of the results (that is, it applies the method \"elementwise\"). So in this case, we'll get back 1,010 matrices of absolute values.\n",
    "\n",
    "Finally, our function calls `mean((-1,-2))`. The tuple `(-1,-2)` represents a range of axes. In Python, `-1` refers to the last element, and `-2` refers to the second-to-last. So in this case, this tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor. The last two axes are the horizontal and vertical dimensions of an image. After taking the mean over the last two axes, we are left with just the first tensor axis, which indexes over our images, which is why our final size was `(1010)`. In other words, for every image, we averaged the intensity of all the pixels in that image.\n",
    "\n",
    "We'll be learning lots more about broadcasting throughout this book, especially in <<chapter_foundations>>, and will be practicing it regularly too.\n",
    "\n",
    "We can use `mnist_distance` to figure out whether an image is a 3 or not by using the following logic: if the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it's a 3. This function will automatically do broadcasting and be applied elementwise, just like all PyTorch functions and operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_3(x): return mnist_distance(x,mean3) < mnist_distance(x,mean7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on our example case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(1.))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_3(a_3), is_3(a_3).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we convert the Boolean response to a float, we get `1.0` for `True` and `0.0` for `False`. Thanks to broadcasting, we can also test it on the full validation set of 3s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_3(valid_3_tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9168), tensor(0.9854), tensor(0.9511))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_3s =      is_3(valid_3_tens).float() .mean()\n",
    "accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n",
    "\n",
    "accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty good start! We're getting over 90% accuracy on both 3s and 7s, and we've seen how to define a metric conveniently using broadcasting.\n",
    "\n",
    "But let's be honest: 3s and 7s are very different-looking digits. And we're only classifying 2 out of the 10 possible digits so far. So we're going to need to do better!\n",
    "\n",
    "To do better, perhaps it is time to try a system that does some real learning—that is, that can automatically modify itself to improve its performance. In other words, it's time to talk about the training process, and SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='15687680' class='' max='15683414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.03% [15687680/15683414 00:08<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the `Image` class from the *Python Imaging Library* (PIL), which is the most widely used Python package for opening, manipulating, and viewing images. Jupyter knows about PIL images, so it displays the image for us automatically.\n",
    "\n",
    "In a computer, everything is represented as a number. To view the numbers that make up this image, we have to convert it to a *NumPy array* or a *PyTorch tensor*. For instance, here's what a section of the image looks like, converted to a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what's in this directory by using `ls`, a method added by fastai. This method returns an object of a special fastai class called `L`, which has all the same functionality of Python's built-in `list`, plus a lot more. One of its handy features is that, when printed, it displays the count of items, before listing the items themselves (if there are more than 10 items, it just shows the first few):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('training'),Path('testing')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset follows a common layout for machine learning datasets: separate folders for the training set and the validation set (and/or test set). Let's see what's inside the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('training/8'),Path('training/4'),Path('training/7'),Path('training/9'),Path('training/5'),Path('training/6'),Path('training/2'),Path('training/0'),Path('training/1'),Path('training/3')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'training').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a folder of 3s, and a folder of 7s. In machine learning parlance, we say that \"3\" and \"7\" are the *labels* (or targets) in this dataset. Let's take a look in one of these folders (using `sorted` to ensure we all get the same order of files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/'training'/'3').ls().sorted()\n",
    "sevens = (path/'training'/'7').ls().sorted()\n",
    "ones = (path/'training'/'1').ls().sorted()\n",
    "twos = (path/'training'/'2').ls().sorted()\n",
    "fours = (path/'training'/'4').ls().sorted()\n",
    "fives = (path/'training'/'5').ls().sorted()\n",
    "sixes = (path/'training'/'6').ls().sorted()\n",
    "eights = (path/'training'/'8').ls().sorted()\n",
    "nines = (path/'training'/'9').ls().sorted()\n",
    "zeros = (path/'training'/'0').ls().sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect, it's full of image files. Let’s take a look at one now. Here’s an image of a handwritten number 3, taken from the famous MNIST dataset of handwritten numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4nM3Or0sDcRjH8c/pgrfBVBjCgibThiKIyTWbWF1bORhGwxARxH/AbtW0JoIGwzXRYhJhtuFY2q1ocLgbe3sGReTuuWbwkx6+r+/zQ/pncX6q+YOldSe6nG3dn8U/rTQ70L8FCGJUewvxl7NTmezNb8xIkvKugr1HSeMP6SrWOVkoTEuSyh0Gm2n3hQyObMnXnxkempRrvgD+gokzwxFAr7U7YXHZ8x4A/Dl7rbu6D2yl3etcw/F3nZgfRVI7rXM7hMUUqzzBec427x26rkmlkzEEa4nnRqnSOH2F0UUx0ePzlbuqMXAHgN6GY9if5xP8dmtHFfwjuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F79A0851F10>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3_path = threes[1]\n",
    "im3 = Image.open(im3_path)\n",
    "im3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the `Image` class from the *Python Imaging Library* (PIL), which is the most widely used Python package for opening, manipulating, and viewing images. Jupyter knows about PIL images, so it displays the image for us automatically.\n",
    "\n",
    "In a computer, everything is represented as a number. To view the numbers that make up this image, we have to convert it to a *NumPy array* or a *PyTorch tensor*. For instance, here's what a section of the image looks like, converted to a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGLzg6wsGBgabUEZscgJfnzMwMFz7x4oQYoKzYjgZGBhMpbGbKvPthzuDyIN/cdh0Pvn37zvDm28M7th0JvyF2PkZm04exn4GBgYGhr/YJJ3+7mFgYGBg6MEiyW/57xwDAwMDg6gWP4aVrj/+FtjoCd//9+9/JKaDHv379+/jtX///n21w5QMP/fv3/9///+dUMHmFx5xcfHit/9qsMlBwDVkSSbc6iiRxLTzFm6dFxnkjHDqZH/5GacctQAAIr5BiOvUvVcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F79A0EA69D0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im4_path = fours[1]\n",
    "im4 = Image.open(im4_path)\n",
    "im4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5851, 6265)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_tensors = [tensor(Image.open(o)) for o in ones]\n",
    "twos_tensors = [tensor(Image.open(o)) for o in twos]\n",
    "threes_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "fours_tensors = [tensor(Image.open(o)) for o in fours]\n",
    "fives_tensors = [tensor(Image.open(o)) for o in fives]\n",
    "sixes_tensors = [tensor(Image.open(o)) for o in sixes]\n",
    "sevens_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "eights_tensors = [tensor(Image.open(o)) for o in eights]\n",
    "nines_tensors = [tensor(Image.open(o)) for o in nines]\n",
    "zeros_tensors = [tensor(Image.open(o)) for o in zeros]\n",
    "len(three_tensors),len(seven_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note: List Comprehensions: List and dictionary comprehensions are a wonderful feature of Python. Many Python programmers use them every day, including the authors of this book—they are part of \"idiomatic Python.\" But programmers coming from other languages may have never seen them before. There are a lot of great tutorials just a web search away, so we won't spend a long time discussing them now. Here is a quick explanation and example to get you started. A list comprehension looks like this: `new_list = [f(o) for o in a_list if o>0]`. This will return every element of `a_list` that is greater than 0, after passing it to the function `f`. There are three parts here: the collection you are iterating over (`a_list`), an optional filter (`if o>0`), and something to do to each element (`f(o)`). It's not only shorter to write but way faster than the alternative ways of creating the same list with a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also check that one of the images looks okay. Since we now have tensors (which Jupyter by default will print as values), rather than PIL images (which Jupyter by default will display as images), we need to use fastai's `show_image` function to display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJHElEQVR4nO2bXXMSZxuAL1jYXYQsiCYxIWIIjImJ0TaVTu2H41FnnGmPPOtMf0NP+i/6H9oDx+OOOtMjW6cdGz/Sk1ZqxkRDhCTQEAjfsLDsvgeWbbMmxgrEzDtcR5ln+bi5eJ5n7/t+iM0wDPr8g/1tB3DY6Aux0BdioS/EQl+IBcc+1/+fb0G23Qb7M8RCX4iFvhALfSEW+kIs9IVY6Aux0BdiYb/ErGMMw6DVatFsNqlUKmiaRrPZpNls0mg0Xnq8LMtIkoSu6+i6jtPpxOFw4HQ6EQQBWZZxOHoXds+FaJpGvV4nHo/zww8/kMlkSKVSxONxnjx5wr/7MTabjffee4+pqSmq1SqqqjIyMsKxY8eIRCIMDw8zOzuLz+frWbxdF6LrujkDisUilUqFbDbLn3/+yeLiIoVCgfX1dTKZDJVKBVEUEUWRer1OrVZjZWUFh8NhCqlUKmxublIulzl+/DgTExM9FWLbp2P2n2sZVVXZ3t5mdXWV77//nlQqxaNHj8jlcqTTaQzDwDAMZFnG7XYzODhIIBDgyZMnPH/+HLvdjt1uN2eOzWbDZrPh8/nwer1cv36daDT6hh93B7vWMh3PEE3TqNVq1Go1UqkUlUqFVCrFysoKT58+pVQqIQgCExMTRKNRnE4nkiQhiiIulwtFUfB6vczOzpLJZFheXubZs2eUy2VqtZr5Po1GA1VV0XW905BfScdCVFUlFovx22+/8c0331CpVGg2m7RaLRqNBoFAgGg0ysWLF7l69Soejwe3220+3263Y7PZ0HUdwzC4ceMG165dIxaLkUwmOw3vP9OxEMMwaDQaVKtVSqUS1WoVXdex2+2IokggEGBubo5z587h8/lwOp04nU7z+e0l8e+lJAgCNtvOGe33+wkGg8iy3GnIr6QrQqrVKrVaDVVV0TQNAFEUOXr0KHNzc3zxxRf4fD4URXnpg7ZpjzudTux2+0vXpqenmZmZQVGUTkN+JR0LcTqdhEIhRFEkn8/TbDaBF0Lcbjdzc3N4vV5EUdxTBrzYizRNI5/Pk8/nzRzFZrMhCALDw8OEw2FcLlenIb+SjoVIksTp06cJh8N88MEH5nh7KQiCgCiK+75Oo9GgVCqxtrZGMpmkWq0CmM+PRCJEo9Ed+08v6FhI+1sXBGHH3tC+Zp3+e7G+vs7PP//M77//TqlUQtM0BEEgHA4zPj7OzMwMJ06ceOk9uk1XErP2bHidmbAXd+7c4auvvkLTNHRdx+FwIIoiFy9eJBqN8u6773LixIluhPtKep66WzEMA13XqdfrlMtlM2FbWFgwN2S73U4kEiESifDhhx8SjUZ7vpm2OXAhuq6jaRrZbJbHjx/z448/cvPmTbLZrHm7djgcXLhwgY8//phPP/2UkydPHlh8B1LtGoZBsVhkdXXVrGWSySSrq6ssLS2Ry+Wo1+vAi3zD7/dz+vRpzp8/z8DAQK9D3MGBlf/JZJLvvvuO5eVlHjx4gKqqO1LzNkNDQ0xPT3PhwgUmJyd7fpu1ciBLRtd1CoUCjx49Yn19HVVVzXzFSiaTIRaLcevWLeLxOMeOHcPj8TA2NobX62VwcLCnkg5syWxtbfHgwQM0TTM31t3IZDJkMhnW19dxu90oioLH4+HKlSucP3+eS5cuIcvyK5O8Tuh6+f/SC/y9ZDY2Nrh9+zblcplCoUCxWCSXy5mPi8fjLC0tmT0UURRxOp1mv2RqaorR0VE++ugjzpw5w9mzZ/F6vQiC8Nq5joVdjfZciJVarWbKWFtbM8d/+uknfvnlF1ZXV0mn07s+t91Ri0QifP3110xOTiJJEoIgvEkovemH/FecTieKoiDL8o7O19DQEJcuXWJtbY10Ok0mkyGXy3H//n3i8TjwYrYlEgmq1SrPnz9ncHCQ48ePv6mQXTlwIQ6HA4fDgcvlwuv1muPDw8NMT09TrVbNW/TKygrZbNYUArC5uUkulyMejxMKhTh69Gh34+vqq3VAuxBsd9VlWSYYDBKPx/nrr79IJBJsb28DL2bKxsYG8XicU6dOdTWOQ3Mu0y4EJUkye63BYJDZ2VmmpqZ2pO6GYZgzR1XVrsZxaITsRbtwtI55vd6eVL+HXgjAbndCt9uN3+/v6oYKh2gPsVIsFtne3mZhYYGHDx/uyFlsNhunTp0iEol01HLYjUMppF0MJpNJEokEiURiR2YrCAJDQ0P4/f6uH2seOiHVapVKpcK9e/e4c+cOf/zxh3lEATA5Ocn4+DiBQABZlt80S92TQyOk/YHr9TrZbJZYLMb8/DypVMq8ZrfbCQaDRCIRvF4vDoej6zXNoRFSKBRIp9Pcvn2bu3fv8vjxY5LJpNknGRgYwO12c/XqVS5fvszo6Oiu5zed8laFtCthwzDI5/M8ffqUhw8fcuPGDbO3Ci820SNHjjA4OMi5c+cIhUI9kQFvUYiqqqiqysbGBktLS9y9e5f5+XkSiQTNZtNcJi6XC1mW+fLLL/nkk08Ih8M9kwFdFvLvb9yaULXH2383Gg0KhQLxeJz5+XkWFha4d++e+fj2me/AwACKonD27FneeecdPB5Pz2RAF4Vomka1WqVcLrO2toaiKIyMjJiH3pVKha2tLUqlEpubmywvL7O4uGjWKfl8HvgnM41EIoTDYa5cuUI0GiUUCqEoSk9/PQRdFNJqtSiVSmxtbRGLxRgdHUWSJPMXRLlcjuXlZba2tsxl0u6tqqq645RPFEXGx8cJh8NEo1FmZmbMhlGv6ZqQfD7Pt99+SzKZ5P79++Zhd6vVotVqmecwqqqaM6ZSqZgb5/DwMGNjY7z//vtmk/nkyZMoioIkSV3PN/aia0IajQaJRIJnz56xuLj4Wj9ssdlsSJKEJEmMjY0RiUQ4c+YM0WiUiYkJ/H5/t8J7bbomxOPxcPnyZTweD7/++uu+QmRZ5siRI3z++ed89tlnhEIhRkZGcLlcB7Y8dqNrQhwOB8FgkHQ6TSAQMI8l98LlcuHz+ZicnGR2dpahoaEdHbS3RdeazK1WyzxvKRaL+7/x3w0ht9ttdsm6XcrvF8KugwfddT9E9P+j6nXoC7HQF2KhL8TCfrfd3lVRh5T+DLHQF2KhL8RCX4iFvhALfSEW/gcMlBno19ugeQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(threes_tensors[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zeros_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this we first combine all the images in this list into a single three-dimensional tensor. The most common way to describe such a tensor is to call it a *rank-3 tensor*. We often need to stack up individual tensors in a collection into a single tensor. Unsurprisingly, PyTorch comes with a function called `stack` that we can use for this purpose.\n",
    "\n",
    "Some operations in PyTorch, such as taking a mean, require us to *cast* our integer types to float types. Since we'll be needing this later, we'll also cast our stacked tensor to `float` now. Casting in PyTorch is as simple as typing the name of the type you wish to cast to, and treating it as a method.\n",
    "\n",
    "Generally when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_sevens = torch.stack(sevens_tensors).float()/255\n",
    "stacked_threes = torch.stack(threes_tensors).float()/255\n",
    "stacked_ones = torch.stack(ones_tensors).float()/255\n",
    "stacked_twos = torch.stack(twos_tensors).float()/255\n",
    "stacked_fours = torch.stack(fours_tensors).float()/255\n",
    "stacked_fives = torch.stack(fives_tensors).float()/255\n",
    "stacked_sixes = torch.stack(sixes_tensors).float()/255\n",
    "stacked_eights = torch.stack(eights_tensors).float()/255\n",
    "stacked_nines = torch.stack(nines_tensors).float()/255\n",
    "stacked_zeros = torch.stack(zeros_tensors).float()/255\n",
    "\n",
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stacked_threes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this dataset, this is the ideal number 3! (You may not like it, but this is what peak number 3 performance looks like.) You can see how it's very dark where all the images agree it should be dark, but it becomes wispy and blurry where the images disagree. \n",
    "\n",
    "Let's do the same thing for the 7s, but put all the steps together at once to save some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1145)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_sevens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean7 = stacked_sevens.mean(0)\n",
    "show_image(mean7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean0 = stacked_zeros.mean(0)\n",
    "show_image(mean0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean1 = stacked_ones.mean(0)\n",
    "show_image(mean1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJGklEQVR4nO1baW+bXBccdgw2dpxEjar+/5/WVqkccNj398OrOb3c4mwmix75SBY2Acwd5szZHGMcR1zsr5mffQNfzS6AaHYBRLMLIJpdANHMfubv/+UQZMztvDBEswsgml0A0ewCiGbPieqn21OlhWHM6uJZ9qmAnFrsS+srHqcDcw5QHwKIusCn3vPzSwBRF833hmHAMAyM4zjZ9xp7N0DmFsvXMAxP7tP/Dvxd7KmXaZqTz5ZlyX6e/xJbHBD9KasLHIbhn1ff9/+81ON5DS6YW8uyYJqmvGzblq3OjtewZVFA5p66uvC2bdE0Deq6Rtu2qKoKdV2jqir53LYt2rYVcHgt0zRhWRZc14VlWfB9H7Ztw/d9WJaFIAhg2/Zky+N5rupK7w6IrgEqIF3Xoes6lGWJuq5RFAWqqkJRFMjzHFmWoSzLfwBRr0kX8DwPtm1jtVrBdV1sNht4noe+7+E4DgzDgOM4sG1btgCEWc/ZIoC8BIiyLJGmKbIsw+FwQJIkOBwOSNMUj4+PqKoKZVmi67oJGFyEbdvCDMdxsF6v4bouoiiC7/vY7/dYrVa4vr6G7/sYx1HYQ3d6CSiLMoTbU26S5zkeHx/x8PCAOI4RxzGyLEOapqjrGnVdT9wM+MuMYRhENwCgaRoYhoG2bWFZFpqmgWVZ8rnrOrRtK3qk3udToLyLhpAddV2jLEscj0ekaYpfv37hz58/+P37twBD5jRNg67r5Fp8oo7jwDRNeJ4n78dxhOu6MAwDdV3DMAxUVQUAKIoC4ziiqioYhoGu6+ScD9EQ3V10UAhMVVXIskx0oygKNE2Dtm0BQJ4+GUEXcRxHxJHa4TgOwjCE4zhYrVbwPA+e58H3fRHd14bbxQCZA6Xve3RdJ8KZZRkeHh5wOBwQxzGSJEFZlsiyTM6lWLquC8dxRBAJBLdcNLUkCAJ4nofNZjPZT0BV+9SwS3Y0TYOyLFEUhYgrw+44jsICz/NkQXzijuNMgOCWzGB4JTDqcQSWbHlpLrJ42KWYMroURYEkSRDHMY7HIx4fH5FlmYgmQViv1wjDULaqK/AYfcEE0zRNyTe4n4zTAXnO3iVTZVLFCJNlGfI8R57nwhI1xfY8D0EQYLPZYLPZTEBhvkGmqDmGGlK5cH5WE7LX6MlZgJwSVLpKVVU4Ho+iG3EcI01TJEkii6VARlGE6+trXF1dIYoirNdr+L4vLkJA+NTVl5rSc/FqWv9pDFFdpu97yT2YfBGQNE0xjiNs2xbX0esSNeoQCIotXWUOiKeKvg8BRGcHGVLXNbIsEyDiOMbPnz9RVRXyPMcwDHAcB33fy0K4cJUFBEEVWf5Nr3B5HX37qcXdnKgSmCRJ0DQNiqKQNJw5SZZl8H0fpmlOWAYAfd9Ln0N1ESZaOiingHmJLR5luBCKKcG4v7/H4XCQ4o1Z6PF4lALNcRwMwyBZK1P5rusEEJb3zDHmeiM6KK+xd2kQzWWsatMHwIRFeZ5L+ByGQapdVr9hGMp+wzAmjOGiqSHAF2gh6i0/lS1ciFrSAxAm5HmOJEkEDDKjaRoEQYC6rhEEgewHgDAMhSG6mJ5riwCi+imjBHsV6/Ua+/0ebdsiyzLJYH3fl3DLLJNPv21bKdpM00Tf91LJOo4DALKlpuhMeSs4i7mMGuYYIsMwxGazwW63wzAMyPNcchRmoUEQSPbJMp+lO69LDWnbFqvVCgDg+z4Mw5CqlyKr2kuqW90W1RCKneu6CMMQ19fXGIYBRVFgv98jiiIRXQKnh1k1r2Dbj+1FAMiyDKZpIgxDaQvwWOALMGQuEXIcB77vI4oiebJpmiKKook4qomTKrbUH1WLWAzmeQ7TNLHdbmHbtjSEyDA1e/6UKKOPCwBI+IyiSMIps1a1eUwAKLBqo5kVMcWYzaOqquC6LpqmkcSO11ABeastHmX41F3XlbzBtu1/WnpzbUY2j8qyFOaoLcW2baXPwvfswQ7DMEnWPpwhc6MG5htcNCMAtUAfQKkCWpal1CkABCT1uyiwBJFAzA2+eN6HiqoOijpkAjAp1nhjOiB0C/6dWW5ZlgKOykD1e+YAOdcWAYQ+XhTFhL4qQ9St6utqgaazhm5B4VRL+7l7WcLO1hBVB6qqEoDUjvlceU4R5L65mY7KBB6nXmPOdKa81m3O1hDgL83zPJ9kmRRUDoyYRPEGGV14LtP4JEmkMKSAqpmo3qHXS/xzCrxFwi5dhsMmJk/sZzDl5oxEFVkCwj6J2mqs63oyySMgKihzPZG3grEIIKoxWtzf30uUYF3juq60DF3XBfB/MAkix5wcanHWSyO7bNuWxrPaVnwOnJfamwDh09K/nFEjz3NxA7W+UfujPI8MUQffHIbrYqq6odpT1YH48PJf91G2+XzflxK9rms8PDyIpvBYvfGr9l/JKuYfAGQQxVFFFEXY7XYIwxBBEAhT5lzowwBRwdArXD49FmWc3TJa6HWPmoWqYVYdXfL6nufJvIaDqLnJ/mvbhosCot7sdruFaZr48eMHPM9DURQwTRNZlqFtW8RxLL8EoBgD09DIhQZBgDAMsd/vsd1ucXd3h9vbW3z79g3r9RqbzUamfKrrvGbksDggBIVJl+d56LoO6/UadV0jDEPUdS2ZKiMR+yIEBMBk3MBayPd9BEEgbrLb7SbDK31UeW4/9SxA1C9kis3O1ziO8gQPhwN2ux2SJMHV1RWKosDxeJRQS5ZxKMUu283NDbbbLb5//46bmxvc3NzIRE+d8p8aR7zVFgm7rDL586UgCAAAV1dXsG0bdV3DcZzJr3oooFwEO2jb7RabzQa3t7fY7/e4u7uTESfZoQ6ynprNvMWMZ2qAZwsEtbJUm8rsqDdNgzRN5fch/BmEDgjFMooirFYrYYI+230uIXsFGLMHLqIhFEXmC9QBhmTTNBEEAYIgEFFldssFMT8hAGrOMieac63Cc90FWIAhk4NnfkWkb+dKdT0Uq1t1v3qseu4b7X0YMvkGrXqd64GcqkRPFWindGEJNsyuYUmG/HPyzFjgyZt5YpHvAMDsBd8VkOfsrX3Phezr/c/dJ4Jx0i7/UaXZc6L69R7hO9uFIZpdANHsAohmF0A0uwCi2QUQzf4HNhhzsE+eQMcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean5 = stacked_fives.mean(0)\n",
    "show_image(mean5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAItElEQVR4nO2baW/aWheFl2eMbYwhobRRVLX//4+1UsrgGU/vh6u138Op07TBJL1XbAlhwuTzeO3xEGMYBtzs/2a+9wn8bXYDotkNiGY3IJrdgGhmv/D8fzkFGWN/vClEs5cUMqlNUfMYxuiFncyuDkSHoD5+7lj9mw6Aj5+7v9SuAmQMwnO3vu/lnse80QzDgGmacq/fDMM4A3MJnKsqhIt6CULXdei6Dm3bouu6MzBcoGVZME0TjuPAsizYti2frwLj970WyuRAdAgARgH0fY+2bXE6neRWlqUc8zVcrOd5cBwHQRCc3RMQcA5GhfUnNimQ5+KArg5CaZoGTdPgdDqhKApUVYW6rlGWpajGtm2YpinvdV0XhmGg6zpYlnX22X+lQgCMAlCVcTqd0DQNqqpCWZbIsgxFUeB4PKIoChRFIUB834fjOIjjGEEQwHVdmKb5k2uNgXmNTQbkOXWox8MwyEKpjLquRRWEUVWVvKfrOnELxhHeqwH1r80yYy6ixwwCSNMUeZ7jx48fSNMUaZridDqhbVsBYFkWPM/DfD7HbDaTe9d1Yds2LMs6g3IpmKtWqmqAVbOJqo6yLCV2MJgOwwDLsuC6rkDwPE9AOI4jsUUNpFOoZBKF/KruIIi2bdE0Deq6Rp7nyLIMu90Ox+MRu90OVVWhqipZ6Gw2QxRFuLu7QxRFWC6X8H0f8/lc1EH3eU4hrwE0qULGYoYaO1RlMLOUZYm6rkUZhmHAcRz4vo8wDDGfz+H7PjzPE3WoMYT2r4ghanoliKIoRB273Q5FUSDPc3mv4zgIwxDr9RpJkmC1WmE+nyMMwzNXGQuol6oDuFIMGYPSdZ0UXWo2YdwAANu2MZvNEIYhwjBEEATwPA+e50kAvUZmUW3ywkyvP5hZqqo6ix2HwwFpmsrzzB7L5RKbzQZJkkjtMZvNRjMK8C9o7nQobduepds8z5HnuaiDccd1XXGNIAjg+77EDLVPUb9nzC4FM2lhpsNgac7YcTgcsN/vcTgcJHbYtg3XdeH7PpIkwXq9xnK5FDCqq/B7njMVxmsr1qsoRC3V27aVajTLMmRZJpml73uYpinqYGplI6f2Jn3fj34f3Yf9DsGxwXv35m4su7BPYd1xPB7PmrcgCJAkCZIkQRRFmM/n4ioABIZa6HGhanc7xUxkcpfhsQqEDVye5yiKAqfTSWYcnuchiiKEYSjB0zAMiT/6DIVGEHy9moF4Dq9RycVAdBCquxDGfr+XzJLnOcqylEKLsYPK4ALbthUoVJM64yAA13WlzKeiVDDv5jJj2YXplk1cnudSlbI8930fvu/LkKdpGgnI/Iymac4qWYJwHOdMVSzaCOVdgdCGYZBUy9rjeDwiTVNkWSbPEUgQBIiiCLZtC4iu66RwY3lP1XmeB8uypHKNogie58EwjDOVMGDznH4XzOQKoTo4AMrzHGmaygIpd1alnHpVVYWu62RotN/vkWWZdMJc0Gw2g+M4qKoKvu+Le7muCwByPxZs3wyIPkdl/FDVURQF6rqGbduSVjkNG4ZB5iNPT0/Y7XYChK7D2QgLtrquEQQBLMtC13WYzWYA/nE5AKI4uuKbAtFhMLswblD2TdPAtm25maaJ0+kkwZbDov1+L+/hZ9NVmIHyPEff9wiCAIZhIAxDgfPcrPVNgfDE1YBKGCzVm6YRyXNxBEIQVAeDKV/b970Ua6xfCIZgbdsWNemjiHeJIXpDp847mD0AyFWu6xpVVaHveynr8zxH27awbRuO40h61rtdumVVVXBdV9L02LDqXWOI3sNwGNQ0zVkdQdfi84fDAYfDQeoOx3GkpPc87wwEM5lhGOKerFXUSfxrbPKpO1XCEyUcdVeOV7coCnkdXYJ1RRAEAsO2bRk+Mxbxc9TdvSnmq1fpZXi12NwRhj4SqKpKUjSnZSzU4jhGGIYSfNM0lZRMGLq9NEX7HZsUiH5l1KaMV5euAEBSomma8H0fpmlK28/ehgrj6JH1DFMu07gaa1i5qjPX37WrjBD1uYTa/TKuMHboI4AwDLFYLGRIpAbQsixl/5ewGXwJYmzM+C5pV/VhXjU1xXZdhzRN5YonSQLDMKSfse1/TuV0OiHLMhyPRwzDIDUKaxMOmF3XRRiGiOMYSZKcTdn0zvdPbBIg6pWgjNUrx73Yuq5FMYTGnX01GA/DIAra7XbIsgxpmqIsS0RRBNd1pVPmzFUdN14yiJ5UIZZlSabgBCzPc4RhKBvYeZ7jcDjIIheLBZbLJVzXhed5kq3oVizq+NMHutR2u8X9/T3u7+9lGD02nee5vSkQPe2xoOLVo+sMwyDFFLMQF+55Hnzfl4zExo6PV6uVqIIQkyRBGIbiKr/as3lTIITCknk+n6Pve3z69AkAcDgcYJomdrsdnp6epK3nlgRdSx0m8ziOY6xWKzw+PiJJEnz9+hUfPnzAly9fEMfx2RxW/YXAu9Uh+m6ZrpDVaoXVaoU8z7Fer1FVFUzTRNM0stHN93FBtm3LIj3PE0Xc3d1hu91iu91iuVxK8abCvGRaNgkQHQYA+ZXPMAwyDdtsNoiiCN++fcPDwwOenp7w/ft36XsIg7t22+0Wi8UCHz9+xHq9xufPn7HZbHB/f48wDM+Cq/4DvNfCmAyIOojhlabrAMB6vQYACY593yMMQ7iue7Y/Y9s2kiQRNSRJgsfHR2w2Gzw8PCCOY8RxLOpRd/OmgAEAxgtN0B91SGo/w+qUP6Gqqkom72ptwR09LohuwIUvFgvJWuruvwoCeNWW5ugLJwUC/Nz5qr0NN7u5jclU3DSNZAS6AWsLNntqjaH+FOICVbwNEHmjNlbU91dYiKmvBc4r3rFjPZVe4B5vC+TsQ7TplX48dg6/6lgn2ul/PyA/fegfDm+u8TsQPAPkTf8bgnalBU5it/+X0ewGRLMbEM1uQDS7AdHsBkSzl9Lu35sfr2Q3hWh2A6LZDYhmNyCa3YBodgOi2f8AB/Frf08MJSUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean6 = stacked_sixes.mean(0)\n",
    "show_image(mean6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean3 = stacked_threes.mean(0)\n",
    "show_image(mean3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean4 = stacked_fours.mean(0)\n",
    "show_image(mean4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean2 = stacked_twos.mean(0)\n",
    "show_image(mean2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAI+ElEQVR4nO1b2W7bSBAs3qd46IgNJP//Z0lsWBbN+yb3YdG9owkdX7SdLNQAIduRRE6xu7q6hlHmecYl/gv1sy/gT4sLIFJcAJHiAogUF0Ck0J/49/9zC1KW/njJECkugEhxAUSKCyBSXACR4gKIFE+13VVDHiTp93mez35eCkVRfvsq//zaeHdAxAXSwud5xjRN/Coe4zieAaSqKh+KokDTNCiKwr+rqsrvm+d5EaiXxLsBIt99EQha+DAM6PseXdeh73v0fY9hGDAMA79f13VomgbTNKFpGizLgq7rMAwDmqYxQCJQYrwUmNUBWQJiHEcGoes6NE2DsiyR5znyPMfxeESSJEjTFGVZoq5rBs62bRiGge12C9/3cX19Dd/3EccxHMeB67owTZMBkzOI4rnArAqIDIacDW3boq5rFEWBPM9xf3+P4/GIu7s7ZFmGNE1R1zWapuHvsCwLpmmibVv4vo9pmhBFEVRVRd/3UBQF0zSdlQwdLwVjVUBEMAiIYRgwjiOapkFd17zonz9/4vb2Fj9+/ECSJDgej8iyDA8PD2iaBm3b8qIcx4Ft2zgcDvB9H/f39wiCAFmWIY5jzphxHGFZFgCc8YpYQs8BZhVAlsAYxxHjOKLvezRNg6qqkOc5kiTB3d0dTqcTsixDURRo25bvtmEYUFUVmqZBVVVYlsV8MU0Tc05VVZw5uq7DcRxomoZxHPlaXmOPvhmQxzJjGAZO/4eHB6Rpiu/fv3NmpGmKJElQ1zWqqoKmaQjDELqu8yHyAR1936MsS2RZhnme4fs+lxaRMAEI4IxLxC70boDQiWTeEO+mmCFN06DrOozjyCVBPGGaJgzDgGmaDAB9JxGzrutQFIWzT+xO9B45O54DxCqAPEaiwzAwb5RlyWVC5FmWJcZxhGEYsCwLjuMgDEMEQQDP8+A4Dp+DAMzzHHVdc0sWz2OaJrqug67r6PseqqrCNE2+rg8l1aU2K/JI13Vo2xZt22IYBu4IhmHAMAz4vo/NZoM4jhHHMTzPg+u6/N4sy1BVFfOMKNzoPGI2yB3mpfEuJUNttmkaNE2DoihQ1zW6ruO0N00TlmVht9vhcDhgv99ju90yiZJYM02T27EIigjGmntLqw53S9kC4EymE0nqug7LsmDbNjzPg+d5rEKJEKdpQtd1DEbTNFwypE6JcwzD4M8SGb8mW95FqcrzCR0AzsDwPA+bzQZBEMB1XTiOw4sgjijLEkVRoCgKVFXFYND3OI4Dx3HOSJla9u8GwQ8DhBYkDmSkK0SdQF1C5BjDMDibsixjMk6SBGVZomkalvKu62Kz2WCz2bB4M03zrF1/eobQyUUgRJEltlJZtOn6v5cyDAO6rsPNzQ1OpxNubm6QpinzD7VpImIRFLHkxBvy4dJdvBsEAAkk0zRh2zaTJRFi3/fI85zLI89zeJ7HYu54PCJNU2RZhrquuStRie12O+x2O/i+z2CI5fJpw52iKL8MVVQepDNc10XTNLAsC13XYZ5n7hikPGlBVVWhrmskSYI8z5lEd7sdbNtGGIY4HA748uULgiDAZrPhclki1E8pGZG06O5QdozjCM/z0HUdPM9D27YAgLZtmSxFjqEMKcuSJ1zXdRFFEeI4xtXVFa6vrxGGIXzfPwNDLJFPmXYpO8TfKUNovhjHkYWW7/soy5IBOR6PTKgUlCHEBa7rwrZtRFGEq6srfP36FXEcIwxDJlMCU5xuP80xk+8EkSbJZwKEMoTmlGmaeEgriuJMbE3TxDI+jmPs93sGI4qis8xYIs+3+KyrdRn5gohYSZGSu2VZFlRVxTiOKMsSaZridDqxAiXuoUUHQYD9fo/r62scDgfWLGJHER2ytxrN79J2lwhWvFhqu23bskQn4UbgEQhxHGO73XKJ/E54rRGrSHf5gpaAERdAKlQc26lTmabJpUJEGscxc8lbhddTsapjJv9OC6d5JE1T5HnOHYT8EHLKDMNAGIaI4xiHw+EsK4jAxYFOHu9fOuovxbvMMrI30vc9t9mqqtjjUBSFzSHHcTgzwjBEGIZMwgSIfA55mBQ10WtjVZN5ySSSDaI0TdG2LTRNw36/h23bDIZt23yQHNd1ne2EruswDAMTr+y4A2/nk9W3IURPhOxDGtRo34UACYIAYRieDWgk+cW2SiRMB4EuAiFfx6fpkCWnrO971HWNPM/x8PCA29tbnE4n1hu0uWRZFsIwRBRFPKkSR4hCiwAWSZha9FL5/DElIwJCoJD4ovF9mibWGUEQII5j7HY79jdk5Qpgcf9X9FjWdMxW6zJLBjNlCO29kFCjbrLf77mrUGaQtCdP9bHskC3ENToMsILr/pifStuWZP/RtgPpjCAIEEURT6y0l0PAyQuWnw4Qz79mrGoyU4ZQ2ldVxeM+zTa2bbOHYds2cwS1ZvJOCVjatKKsEM+3FI/NNM+NVbuMvI0pPtogmsK0SLpgKgnySOhzMmfIBAq8zC99TrzbNoS8XzLPM29vzvPMGoW6Cc00tH9DHYbaMHBuL5CHIg92f6QOoZ8pRNlNgxx1IloEKVoCUzSYRNElL36pRN4CzKqeKt01mktIltO4XxQFu+wktuRFil2IJmXasqAtB/rev2L8l4HRdZ1dLBr5kyTh8qDP0QLJ76BNLSoZOshnobIRz/vpbVfcdqBHEWzbhuu6UBSFJXpVVVAUhZ8MIl1S1zUvijavfN9njbLb7fDt2zdst1tst1t4ngff9zmLRB55a3dZBRAKeXOKthscx8EwDNhsNui6jq1Ex3G4s1C4rsuGMqlYcer93VbDWyzDX9byhLB5luohMiVCpLZJg1yapqiqCqfTiZ8iIr1BCyKLkcZ/8lR932cXjcpFBuOVmbH45lU9VUp/2qx2XZf1Bk2vURRhu90yuYqAULmRHUBAiFy0EhiPr2ONDOE3LzxAI76Kz3YsCSyxWyy9iu8TP/fKWPzgqoDwhyRBRq9Lf//lgha0xGOe7Rvj4wD55UteOYCtbSDLX7/0xw95+P+dF7ZqXP57iBQXQKR4qmT+nlxfKS4ZIsUFECkugEhxAUSKCyBSXACR4h8k6ydV040kfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean8= stacked_eights.mean(0)\n",
    "show_image(mean8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean9 = stacked_nines.mean(0)\n",
    "show_image(mean9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pick an arbitrary 3 and measure its *distance* from our \"ideal digits.\"\n",
    "\n",
    "> stop: Stop and Think!: How would you calculate how similar a particular image is to each of our ideal digits? Remember to step away from this book and jot down some ideas before you move on! Research shows that recall and understanding improves dramatically when you are engaged with the learning process by solving problems, experimenting, and trying new ideas yourself\n",
    "\n",
    "Here's a sample 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_3 = stacked_threes[1]\n",
    "show_image(a_3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we determine its distance from our ideal 3? We can't just add up the differences between the pixels of this image and the ideal digit. Some differences will be positive while others will be negative, and these differences will cancel out, resulting in a situation where an image that is too dark in some places and too light in others might be shown as having zero total differences from the ideal. That would be misleading!\n",
    "\n",
    "To avoid this, there are two main ways data scientists measure distance in this context:\n",
    "\n",
    "- Take the mean of the *absolute value* of differences (absolute value is the function that replaces negative values with positive values). This is called the *mean absolute difference* or *L1 norm*\n",
    "- Take the mean of the *square* of differences (which makes everything positive) and then take the *square root* (which undoes the squaring). This is called the *root mean squared error* (RMSE) or *L2 norm*.\n",
    "\n",
    "> important: It's Okay to Have Forgotten Your Math: In this book we generally assume that you have completed high school math, and remember at least some of it... But everybody forgets some things! It all depends on what you happen to have had reason to practice in the meantime. Perhaps you have forgotten what a _square root_ is, or exactly how they work. No problem! Any time you come across a maths concept that is not explained fully in this book, don't just keep moving on; instead, stop and look it up. Make sure you understand the basic idea, how it works, and why we might be using it. One of the best places to refresh your understanding is Khan Academy. For instance, Khan Academy has a great [introduction to square roots](https://www.khanacademy.org/math/algebra/x2f8bb11595b61c86:rational-exponents-radicals/x2f8bb11595b61c86:radicals/v/understanding-square-roots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try both of these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1114), tensor(0.2021))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_3_abs = (a_3 - mean3).abs().mean()\n",
    "dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n",
    "dist_3_abs,dist_3_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_7_abs = (a_3 - mean7).abs().mean()\n",
    "dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n",
    "dist_7_abs,dist_7_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the distance between our 3 and the \"ideal\" 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides both of these as *loss functions*. You'll find these inside `torch.nn.functional`, which the PyTorch team recommends importing as `F` (and is available by default under that name in fastai):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `mse` stands for *mean squared error*, and `l1` refers to the standard mathematical jargon for *mean absolute value* (in math it's called the *L1 norm*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_4 = stacked_fours[1]\n",
    "show_image(a_4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1126), tensor(0.2138))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_4_abs = (a_4 - mean4).abs().mean()\n",
    "dist_4_sqr = ((a_4 - mean4)**2).mean().sqrt()\n",
    "dist_4_abs,dist_4_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1246), tensor(0.2471))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_4_abs = (a_4 - mean7).abs().mean()\n",
    "dist_4_sqr = ((a_4 - mean7)**2).mean().sqrt()\n",
    "dist_4_abs,dist_4_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the distance between our 3 and the \"ideal\" 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides both of these as *loss functions*. You'll find these inside `torch.nn.functional`, which the PyTorch team recommends importing as `F` (and is available by default under that name in fastai):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1126), tensor(0.2138))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.l1_loss(a_4.float(),mean4), F.mse_loss(a_4,mean4).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How is a grayscale image represented on a computer? How about a color image?\n",
    "1. How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?\n",
    "1. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "1. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "1. What is a \"rank-3 tensor\"?\n",
    "1. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "1. What are RMSE and L1 norm?\n",
    "1. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "1. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
    "1. What is broadcasting?\n",
    "1. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "1. What is SGD?\n",
    "1. Why does SGD use mini-batches?\n",
    "1. What are the seven steps in SGD for machine learning?\n",
    "1. How do we initialize the weights in a model?\n",
    "1. What is \"loss\"?\n",
    "1. Why can't we always use a high learning rate?\n",
    "1. What is a \"gradient\"?\n",
    "1. Do you need to know how to calculate gradients yourself?\n",
    "1. Why can't we use accuracy as a loss function?\n",
    "1. Draw the sigmoid function. What is special about its shape?\n",
    "1. What is the difference between a loss function and a metric?\n",
    "1. What is the function to calculate new weights using a learning rate?\n",
    "1. What does the `DataLoader` class do?\n",
    "1. Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
    "1. Create a function that, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output data structure?\n",
    "1. What does `view` do in PyTorch?\n",
    "1. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "1. What does the `@` operator do in Python?\n",
    "1. What does the `backward` method do?\n",
    "1. Why do we have to zero the gradients?\n",
    "1. What information do we have to pass to `Learner`?\n",
    "1. Show Python or pseudocode for the basic steps of a training loop.\n",
    "1. What is \"ReLU\"? Draw a plot of it for values from `-2` to `+2`.\n",
    "1. What is an \"activation function\"?\n",
    "1. What's the difference between `F.relu` and `nn.ReLU`?\n",
    "1. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create your own implementation of `Learner` from scratch, based on the training loop shown in this chapter.\n",
    "1. Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You'll need to do some of your own research to figure out how to overcome some obstacles you'll meet on the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
